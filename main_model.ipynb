{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2023-01-25T19:48:09.880006Z",
          "iopub.status.busy": "2023-01-25T19:48:09.879718Z",
          "iopub.status.idle": "2023-01-25T19:48:09.895395Z",
          "shell.execute_reply": "2023-01-25T19:48:09.894685Z",
          "shell.execute_reply.started": "2023-01-25T19:48:09.879973Z"
        },
        "id": "n8mqx8kBVg0L",
        "outputId": "0e220023-d2c4-4159-f2a4-82a2e141318b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting generate_preds.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile generate_preds.py\n",
        "\n",
        "\n",
        "from multiprocessing import set_start_method\n",
        "from multiprocessing import Process\n",
        "\n",
        "\n",
        "# set the start method\n",
        "set_start_method('fork')\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument('--model_paths', nargs='+', required=True)\n",
        "ap.add_argument(\"--save_name\", type=str, required=True)\n",
        "ap.add_argument(\"--max_len\", type=int, required=True)\n",
        "args = ap.parse_args()\n",
        "\n",
        "if args.save_name == \"yoso\":\n",
        "    os.system(\"cp -r ./input/hf-transformers/transformers-4.16.0 .\")\n",
        "    os.system(\"pip install -U --no-build-isolation --no-deps /kaggle/working/transformers-4.16.0\")\n",
        "\n",
        "if (\"v3\" in args.save_name)|(\"v2\" in args.save_name):\n",
        "    # https://www.kaggle.com/nbroad/deberta-v2-3-fast-tokenizer\n",
        "    # The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
        "    # This must be done before importing transformers\n",
        "    import shutil\n",
        "    from pathlib import Path\n",
        "\n",
        "    transformers_path = Path(\"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers\")\n",
        "\n",
        "    input_dir = Path(\"./input/deberta-v2-3-fast-tokenizer\")\n",
        "\n",
        "    convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
        "    conversion_path = transformers_path/convert_file.name\n",
        "\n",
        "    if conversion_path.exists():\n",
        "        conversion_path.unlink()\n",
        "\n",
        "    shutil.copy(convert_file, transformers_path)\n",
        "    deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
        "\n",
        "    for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
        "        filepath = deberta_v2_path/filename\n",
        "        if filepath.exists():\n",
        "            filepath.unlink()\n",
        "\n",
        "        shutil.copy(input_dir/filename, filepath)\n",
        "\n",
        "if args.save_name == \"longformerwithlstm\":\n",
        "    os.system(\"cp -r ./input/longformerwithbilstmhead/model.py .\")\n",
        "    from model import LongformerForTokenClassificationwithbiLSTM\n",
        "\n",
        "if args.save_name == \"debertawithlstm\":\n",
        "    os.system(\"cp -r ./input/deberta-lstm/model.py .\")\n",
        "    from model import DebertaForTokenClassificationwithbiLSTM\n",
        "\n",
        "import gc\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import multiprocessing as mp\n",
        "from scipy.special import softmax\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (AutoModelForTokenClassification,\n",
        "                          AutoTokenizer,\n",
        "                          TrainingArguments,\n",
        "                          Trainer)\n",
        "\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "NUM_CORES = 16\n",
        "BATCH_SIZE = 4\n",
        "MAX_SEQ_LENGTH = args.max_len\n",
        "PRETRAINED_MODEL_PATHS = args.model_paths\n",
        "if \"debertal_chris\" in args.save_name:\n",
        "    print('==> using -1 in offset mapping...')\n",
        "if (\"v3\" in args.save_name)|(\"v2\" in args.save_name):\n",
        "    print('==> using -1 in offset mapping...')\n",
        "\n",
        "AGG_FUNC = np.mean\n",
        "print('==> using span token mean...')\n",
        "\n",
        "TEST_DIR = './feedback-prize-main/input/feedback-prize-2021/train/'\n",
        "\n",
        "MIN_TOKENS = {\n",
        "    \"Lead\": 32,\n",
        "    \"Position\": 5,\n",
        "    \"Evidence\": 35,\n",
        "    \"Claim\": 7,\n",
        "    \"Concluding Statement\": 6,\n",
        "    \"Counterclaim\": 6,\n",
        "    \"Rebuttal\": 6\n",
        "}\n",
        "\n",
        "if \"chris\" not in args.save_name:\n",
        "    ner_labels = {'O': 0,\n",
        "                  'B-Lead': 1,\n",
        "                  'I-Lead': 2,\n",
        "                  'B-Position': 3,\n",
        "                  'I-Position': 4,\n",
        "                  'B-Evidence': 5,\n",
        "                  'I-Evidence': 6,\n",
        "                  'B-Claim': 7,\n",
        "                  'I-Claim': 8,\n",
        "                  'B-Concluding Statement': 9,\n",
        "                  'I-Concluding Statement': 10,\n",
        "                  'B-Counterclaim': 11,\n",
        "                  'I-Counterclaim': 12,\n",
        "                  'B-Rebuttal': 13,\n",
        "                  'I-Rebuttal': 14}\n",
        "else:\n",
        "    print(\"==> Using Chris BIO\")\n",
        "    ner_labels = {'O': 14,\n",
        "                  'B-Lead': 0,\n",
        "                  'I-Lead': 1,\n",
        "                  'B-Position': 2,\n",
        "                  'I-Position': 3,\n",
        "                  'B-Evidence': 4,\n",
        "                  'I-Evidence': 5,\n",
        "                  'B-Claim': 6,\n",
        "                  'I-Claim': 7,\n",
        "                  'B-Concluding Statement': 8,\n",
        "                  'I-Concluding Statement': 9,\n",
        "                  'B-Counterclaim': 10,\n",
        "                  'I-Counterclaim': 11,\n",
        "                  'B-Rebuttal': 12,\n",
        "                  'I-Rebuttal': 13}\n",
        "\n",
        "\n",
        "inverted_ner_labels = dict((v,k) for k,v in ner_labels.items())\n",
        "inverted_ner_labels[-100] = 'Special Token'\n",
        "\n",
        "test_files = os.listdir(TEST_DIR)\n",
        "\n",
        "# accepts file path, returns tuple of (file_ID, txt split, NER labels)\n",
        "def generate_text_for_file(input_filename):\n",
        "    curr_id = input_filename.split('.')[0]\n",
        "    with open(os.path.join(TEST_DIR, input_filename)) as f:\n",
        "        curr_txt = f.read()\n",
        "\n",
        "    return curr_id, curr_txt\n",
        "\n",
        "with mp.Pool(NUM_CORES) as p:\n",
        "    ner_test_rows = p.map(generate_text_for_file, test_files)\n",
        "\n",
        "if (\"v3\" in args.save_name)|(\"v2\" in args.save_name):\n",
        "    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
        "    tokenizer = DebertaV2TokenizerFast.from_pretrained(PRETRAINED_MODEL_PATHS[0])\n",
        "else:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_PATHS[0])\n",
        "# Check is rust-based fast tokenizer\n",
        "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
        "\n",
        "ner_test_rows = sorted(ner_test_rows, key=lambda x: len(tokenizer(x[1], max_length=MAX_SEQ_LENGTH, truncation=True)['input_ids']))\n",
        "\n",
        "# tokenize and store word ids\n",
        "def tokenize_with_word_ids(ner_raw_data):\n",
        "    # ner_raw_data is shaped (num_examples, 3) where cols are (ID, words, word-level labels)\n",
        "    tokenized_inputs = tokenizer([x[1] for x in ner_raw_data],\n",
        "                                 max_length=MAX_SEQ_LENGTH,\n",
        "                                 return_offsets_mapping=True,\n",
        "                                 truncation=True)\n",
        "\n",
        "    tokenized_inputs['id'] = [x[0] for x in ner_raw_data]\n",
        "    tokenized_inputs['offset_mapping'] = [tokenized_inputs['offset_mapping'][i] for i in range(len(ner_raw_data))]\n",
        "\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_all = tokenize_with_word_ids(ner_test_rows)\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, input_dict):\n",
        "        self.input_dict = input_dict\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {k:self.input_dict[k][index] for k in self.input_dict.keys() if k not in {'id', 'offset_mapping'}}\n",
        "\n",
        "    def get_filename(self, index):\n",
        "        return self.input_dict['id'][index]\n",
        "\n",
        "    def get_offset(self, index):\n",
        "        return self.input_dict['offset_mapping'][index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_dict['input_ids'])\n",
        "\n",
        "test_dataset = NERDataset(tokenized_all)\n",
        "\n",
        "soft_predictions = None\n",
        "hfargs = TrainingArguments(output_dir='None',\n",
        "                         log_level='warning',\n",
        "                         per_device_eval_batch_size=BATCH_SIZE)\n",
        "\n",
        "for idx, curr_path in enumerate(PRETRAINED_MODEL_PATHS):\n",
        "\n",
        "    if args.save_name == \"longformerwithlstm\":\n",
        "        model = LongformerForTokenClassificationwithbiLSTM.from_pretrained(curr_path)\n",
        "    elif args.save_name == \"debertawithlstm\":\n",
        "        model = DebertaForTokenClassificationwithbiLSTM.from_pretrained(curr_path)\n",
        "    else:\n",
        "        model = AutoModelForTokenClassification.from_pretrained(curr_path, trust_remote_code=True)\n",
        "    trainer = Trainer(model,\n",
        "                      hfargs,\n",
        "                      tokenizer=tokenizer)\n",
        "\n",
        "    curr_preds, _, _ = trainer.predict(test_dataset)\n",
        "    curr_preds = curr_preds.astype(np.float16)\n",
        "    curr_preds = softmax(curr_preds, -1)\n",
        "\n",
        "    if soft_predictions is not None:\n",
        "        soft_predictions = soft_predictions + curr_preds\n",
        "    else:\n",
        "        soft_predictions = curr_preds\n",
        "\n",
        "    del model, trainer, curr_preds\n",
        "    gc.collect()\n",
        "\n",
        "soft_predictions = soft_predictions / len(PRETRAINED_MODEL_PATHS)\n",
        "\n",
        "soft_claim_predictions = soft_predictions[:, :, 8]\n",
        "\n",
        "predictions = np.argmax(soft_predictions, axis=2)\n",
        "soft_predictions = np.max(soft_predictions, axis=2)\n",
        "\n",
        "def generate_token_to_word_mapping(txt, offset):\n",
        "    # GET WORD POSITIONS IN CHARS\n",
        "    w = []\n",
        "    blank = True\n",
        "    for i in range(len(txt)):\n",
        "        if not txt[i].isspace() and blank==True:\n",
        "            w.append(i)\n",
        "            blank=False\n",
        "        elif txt[i].isspace():\n",
        "            blank=True\n",
        "    w.append(1e6)\n",
        "\n",
        "    # MAPPING FROM TOKENS TO WORDS\n",
        "    word_map = -1 * np.ones(len(offset),dtype='int32')\n",
        "    w_i = 0\n",
        "    for i in range(len(offset)):\n",
        "        if offset[i][1]==0: continue\n",
        "        while offset[i][0]>=(w[w_i+1]-(\"debertal_chris\" in args.save_name)-(\"v3\" in args.save_name)\\\n",
        "                             -(\"v2\" in args.save_name) ): w_i += 1\n",
        "        word_map[i] = int(w_i)\n",
        "\n",
        "    return word_map\n",
        "\n",
        "all_preds = []\n",
        "\n",
        "# Clumsy gathering of predictions at word lvl - only populate with 1st subword pred\n",
        "for curr_sample_id in range(len(test_dataset)):\n",
        "    curr_preds = []\n",
        "    sample_preds = predictions[curr_sample_id]\n",
        "    sample_offset = test_dataset.get_offset(curr_sample_id)\n",
        "    sample_txt = ner_test_rows[curr_sample_id][1]\n",
        "    sample_word_map = generate_token_to_word_mapping(sample_txt, sample_offset)\n",
        "\n",
        "    word_preds = [''] * (max(sample_word_map) + 1)\n",
        "    word_probs = dict(zip(range((max(sample_word_map) + 1)),[0]*(max(sample_word_map) + 1)))\n",
        "    claim_probs = dict(zip(range((max(sample_word_map) + 1)),[0]*(max(sample_word_map) + 1)))\n",
        "\n",
        "    for i, curr_word_id in enumerate(sample_word_map):\n",
        "        if curr_word_id != -1:\n",
        "            if word_preds[curr_word_id] == '': # only use 1st subword\n",
        "                word_preds[curr_word_id] = inverted_ner_labels[sample_preds[i]]\n",
        "                word_probs[curr_word_id] = soft_predictions[curr_sample_id, i]\n",
        "                claim_probs[curr_word_id] = soft_claim_predictions[curr_sample_id, i]\n",
        "            elif 'B-' in inverted_ner_labels[sample_preds[i]]:\n",
        "                word_preds[curr_word_id] = inverted_ner_labels[sample_preds[i]]\n",
        "                word_probs[curr_word_id] = soft_predictions[curr_sample_id, i]\n",
        "                claim_probs[curr_word_id] = soft_claim_predictions[curr_sample_id, i]\n",
        "\n",
        "    # Dict to hold Lead, Position, Concluding Statement\n",
        "    let_one_dict = dict() # K = Type, V = (Prob of start token, start, end)\n",
        "\n",
        "    # If we see tokens I-X, I-Y, I-X -> change I-Y to I-X\n",
        "    for j in range(1, len(word_preds) - 1):\n",
        "        pred_trio = [word_preds[k] for k in [j - 1, j, j + 1]]\n",
        "        splitted_trio = [x.split('-')[0] for x in pred_trio]\n",
        "        if all([x == 'I' for x in splitted_trio]) and pred_trio[0] == pred_trio[2] and pred_trio[0] != pred_trio[1]:\n",
        "            word_preds[j] = word_preds[j-1]\n",
        "\n",
        "    # B-X, ? (not B), I-X -> change ? to I-X\n",
        "    for j in range(1, len(word_preds) - 1):\n",
        "        if 'B-' in word_preds[j-1] and word_preds[j+1] == f\"I-{word_preds[j-1].split('-')[-1]}\" and word_preds[j] != word_preds[j+1] and 'B-' not in word_preds[j]:\n",
        "            word_preds[j] = word_preds[j+1]\n",
        "\n",
        "     # If we see tokens I-X, O, I-X, change center token to the same for stated discourse types\n",
        "    for j in range(1, len(word_preds) - 1):\n",
        "        if word_preds[j - 1] in ['I-Lead', 'I-Position', 'I-Concluding Statement'] and word_preds[j-1] == word_preds[j+1] and word_preds[j] == 'O':\n",
        "            word_preds[j] = word_preds[j-1]\n",
        "\n",
        "    j = 0 # start of candidate discourse\n",
        "    while j < len(word_preds):\n",
        "        cls = word_preds[j]\n",
        "        cls_splitted = cls.split('-')[-1]\n",
        "        end = j + 1 # try to extend discourse as far as possible\n",
        "\n",
        "        if word_probs[j] > 0.54:\n",
        "            # Must match suffix i.e., I- to I- only; no B- to I-\n",
        "            while end < len(word_preds) and (word_preds[end].split('-')[-1] == cls_splitted if cls_splitted in ['Lead', 'Position', 'Concluding Statement'] else word_preds[end] == f'I-{cls_splitted}'):\n",
        "                end += 1\n",
        "            # if we're here, end is not the same pred as start\n",
        "            if cls != 'O' and (end - j > MIN_TOKENS[cls_splitted] or max(word_probs[l] for l in range(j, end)) > 0.73): # needs to be longer than class-specified min\n",
        "                if cls_splitted in ['Lead', 'Position', 'Concluding Statement']:\n",
        "                    lpc_max_prob = max(word_probs[c] for c in range(j, end))\n",
        "                    if cls_splitted in let_one_dict: # Already existing, check contiguous or higher prob\n",
        "                        prev_prob, prev_start, prev_end = let_one_dict[cls_splitted]\n",
        "                        if cls_splitted in ['Lead', 'Concluding Statement'] and j - prev_end < 49: # If close enough, combine\n",
        "                            let_one_dict[cls_splitted] = (max(prev_prob, lpc_max_prob), prev_start, end)\n",
        "\n",
        "                            # Delete other preds that lie inside the joined LC discourse\n",
        "                            for l in range(len(curr_preds) - 1, 0, -1):\n",
        "                                check_span = curr_preds[l][2]\n",
        "                                check_start, check_end = int(check_span[0]), int(check_span[-1])\n",
        "                                if check_start > prev_start and check_end < end:\n",
        "                                    del curr_preds[l]\n",
        "\n",
        "                        elif lpc_max_prob > prev_prob: # Overwrite if current candidate is more likely\n",
        "                            let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\n",
        "                    else: # Add to it\n",
        "                        let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\n",
        "                else:\n",
        "                    # Lookback and add preceding I- tokens\n",
        "                    while j - 1 > 0 and word_preds[j-1] == cls:\n",
        "                        j = j - 1\n",
        "                    # Try to add the matching B- tag if immediately precedes the current I- sequence\n",
        "                    if j - 1 > 0 and word_preds[j-1] == f'B-{cls_splitted}':\n",
        "                        j = j - 1\n",
        "\n",
        "\n",
        "                    #############################################################\n",
        "                    # Run a bunch of adjustments to discourse predictions based on CV\n",
        "                    adj_start, adj_end = j, end + 1\n",
        "\n",
        "                    # Run some heuristics against previous discourse\n",
        "                    if len(curr_preds) > 0:\n",
        "                        prev_span = list(map(int, curr_preds[-1][2].split()))\n",
        "                        prev_start, prev_end = prev_span[0], prev_span[-1]\n",
        "\n",
        "                        # Join adjacent rebuttals\n",
        "                        if cls_splitted in 'Rebuttal':\n",
        "                            if curr_preds[-1][1] == cls_splitted and adj_start - prev_end < 32:\n",
        "                                del curr_preds[-1]\n",
        "                                combined_list = prev_span + list(range(adj_start, adj_end))\n",
        "                                curr_preds.append((test_dataset.get_filename(curr_sample_id),\n",
        "                                                   cls_splitted,\n",
        "                                                   ' '.join(map(str, combined_list)),\n",
        "                                                   AGG_FUNC([word_probs[i] for i in combined_list if i in word_probs.keys()])))\n",
        "                                j = end\n",
        "                                continue\n",
        "\n",
        "                        elif cls_splitted in 'Counterclaim':\n",
        "                            if curr_preds[-1][1] == cls_splitted and adj_start - prev_end < 24:\n",
        "                                del curr_preds[-1]\n",
        "                                combined_list = prev_span + list(range(adj_start, adj_end))\n",
        "                                curr_preds.append((test_dataset.get_filename(curr_sample_id),\n",
        "                                                   cls_splitted,\n",
        "                                                   ' '.join(map(str, combined_list)),\n",
        "                                                  AGG_FUNC([word_probs[i] for i in combined_list if i in word_probs.keys()])))\n",
        "                                j = end\n",
        "                                continue\n",
        "\n",
        "                        elif cls_splitted in 'Evidence':\n",
        "                            if curr_preds[-1][1] == cls_splitted and 8 < adj_start - prev_end < 25:\n",
        "                                if max(claim_probs[l] for l in range(prev_end+1, adj_start)) > 0.35:\n",
        "                                    claim_tokens = [str(l) for l in range(prev_end+1, adj_start) if claim_probs[l] > 0.15]\n",
        "                                    if len(claim_tokens) > 2:\n",
        "                                        curr_preds.append((test_dataset.get_filename(curr_sample_id),\n",
        "                                                           'Claim',\n",
        "                                                           ' '.join(claim_tokens),\n",
        "                                                           AGG_FUNC([word_probs[int(i)] for i in claim_tokens if int(i) in word_probs.keys()])))\n",
        "                        # If gap with discourse of same type, extend to it\n",
        "                        elif curr_preds[-1][1] == cls_splitted and adj_start - prev_end > 2:\n",
        "                            adj_start -= 1\n",
        "\n",
        "                    # Adjust discourse lengths if too long or short\n",
        "                    if cls_splitted == 'Evidence':\n",
        "                        if adj_end - adj_start < 45:\n",
        "                            adj_start -= 9\n",
        "                        else:\n",
        "                            adj_end -= 1\n",
        "                    elif cls_splitted == 'Claim':\n",
        "                        if adj_end - adj_start > 24:\n",
        "                            adj_end -= 1\n",
        "                    elif cls_splitted == 'Counterclaim':\n",
        "                        if adj_end - adj_start > 24:\n",
        "                            adj_end -= 1\n",
        "                        else:\n",
        "                            adj_start -= 1\n",
        "                            adj_end += 1\n",
        "                    elif cls_splitted == 'Rebuttal':\n",
        "                        if adj_end - adj_start > 32:\n",
        "                            adj_end -= 1\n",
        "                        else:\n",
        "                            adj_start -= 1\n",
        "                            adj_end += 1\n",
        "                    adj_start = max(0, adj_start)\n",
        "                    adj_end = min(len(word_preds) - 1, adj_end)\n",
        "                    curr_preds.append((test_dataset.get_filename(curr_sample_id),\n",
        "                                       cls_splitted,\n",
        "                                       ' '.join(map(str, list(range(adj_start, adj_end)))),\n",
        "                                       AGG_FUNC([word_probs[i] for i in range(adj_start, adj_end) if i in word_probs.keys()])))\n",
        "\n",
        "        j = end\n",
        "\n",
        "    # Add the Lead, Position, Concluding Statement\n",
        "    for k, v in let_one_dict.items():\n",
        "        pred_start = v[1]\n",
        "        pred_end = v[2]\n",
        "\n",
        "        # Lookback and add preceding I- tokens\n",
        "        while pred_start - 1 > 0 and word_preds[pred_start-1] == f'I-{k}':\n",
        "            pred_start = pred_start - 1\n",
        "        # Try to add the matching B- tag if immediately precedes the current I- sequence\n",
        "        if pred_start - 1 > 0 and word_preds[pred_start - 1] == f'B-{k}':\n",
        "            pred_start = pred_start - 1\n",
        "\n",
        "        # Extend short Leads and Concluding Statements\n",
        "        if k == 'Lead':\n",
        "            if pred_end - pred_start < 33:\n",
        "                pred_end = min(len(word_preds), pred_end + 5)\n",
        "            else:\n",
        "                pred_end -= 5\n",
        "        elif k == 'Concluding Statement':\n",
        "            if pred_end - pred_start < 23:\n",
        "                pred_start = max(0, pred_start - 1)\n",
        "                pred_end = min(len(word_preds), pred_end + 10)\n",
        "        elif k == 'Position':\n",
        "            if pred_end - pred_start < 18:\n",
        "                pred_end = min(len(word_preds), pred_end + 3)\n",
        "\n",
        "        pred_start = max(0, pred_start)\n",
        "        if pred_end - pred_start > 6:\n",
        "            curr_preds.append((test_dataset.get_filename(curr_sample_id),\n",
        "                               k,\n",
        "                               ' '.join(map(str, list(range(pred_start, pred_end)))),\n",
        "                               AGG_FUNC([word_probs[i] for i in range(pred_start, pred_end) if i in word_probs.keys()])))\n",
        "\n",
        "    all_preds.extend(curr_preds)\n",
        "\n",
        "output_df = pd.DataFrame(all_preds)\n",
        "output_df.columns = ['id', 'class', 'predictionstring', 'scores']\n",
        "output_df.to_csv(f'{args.save_name}.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaihMR8lVg0P"
      },
      "source": [
        "### Weighted Box Fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-03-13T07:39:00.315488Z",
          "iopub.status.busy": "2022-03-13T07:39:00.315048Z",
          "iopub.status.idle": "2022-03-13T07:39:00.434581Z",
          "shell.execute_reply": "2022-03-13T07:39:00.433364Z",
          "shell.execute_reply.started": "2022-03-13T07:39:00.315448Z"
        },
        "id": "tkTEAynvVg0Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "\n",
        "def prefilter_boxes(boxes, scores, labels, weights, thr):\n",
        "    # Create dict with boxes stored by its label\n",
        "    new_boxes = dict()\n",
        "\n",
        "    for t in range(len(boxes)):\n",
        "\n",
        "        if len(boxes[t]) != len(scores[t]):\n",
        "            print('Error. Length of boxes arrays not equal to length of scores array: {} != {}'.format(len(boxes[t]), len(scores[t])))\n",
        "            exit()\n",
        "\n",
        "        for j in range(len(boxes[t])):\n",
        "\n",
        "            score = scores[t][j]\n",
        "            if score < thr:\n",
        "                continue\n",
        "            label = labels[t][j]\n",
        "            box_part = boxes[t][j]\n",
        "\n",
        "            x = float(box_part[0])\n",
        "            y = float(box_part[1])\n",
        "\n",
        "            # Box data checks\n",
        "            if y < x:\n",
        "                warnings.warn('Y < X value in box. Swap them.')\n",
        "                x, y = y, x\n",
        "\n",
        "            # [label, score, weight, model index, x, y]\n",
        "            b = [label, float(score) * weights[t], weights[t], t, x, y]\n",
        "            if label not in new_boxes:\n",
        "                new_boxes[label] = []\n",
        "            new_boxes[label].append(b)\n",
        "\n",
        "    # Sort each list in dict by score and transform it to numpy array\n",
        "    for k in new_boxes:\n",
        "        current_boxes = np.array(new_boxes[k])\n",
        "        new_boxes[k] = current_boxes[current_boxes[:, 1].argsort()[::-1]]\n",
        "\n",
        "    return new_boxes\n",
        "\n",
        "\n",
        "def get_weighted_box(boxes, conf_type='avg'):\n",
        "    \"\"\"\n",
        "    Create weighted box for set of boxes\n",
        "    :param boxes: set of boxes to fuse\n",
        "    :param conf_type: type of confidence one of 'avg' or 'max'\n",
        "    :return: weighted box (label, score, weight, model index, x, y)\n",
        "    \"\"\"\n",
        "\n",
        "    box = np.zeros(6, dtype=np.float32)\n",
        "    conf = 0\n",
        "    conf_list = []\n",
        "    w = 0\n",
        "    for b in boxes:\n",
        "        box[4:] += (b[1] * b[4:])\n",
        "        conf += b[1]\n",
        "        conf_list.append(b[1])\n",
        "        w += b[2]\n",
        "    box[0] = boxes[0][0]\n",
        "    if conf_type == 'avg':\n",
        "        box[1] = conf / len(boxes)\n",
        "    elif conf_type == 'max':\n",
        "        box[1] = np.array(conf_list).max()\n",
        "    elif conf_type in ['box_and_model_avg', 'absent_model_aware_avg']:\n",
        "        box[1] = conf / len(boxes)\n",
        "    box[2] = w\n",
        "    box[3] = -1 # model index field is retained for consistensy but is not used.\n",
        "    box[4:] /= conf\n",
        "    return box\n",
        "\n",
        "\n",
        "def find_matching_box_quickly(boxes_list, new_box, match_iou):\n",
        "    \"\"\"\n",
        "        Reimplementation of find_matching_box with numpy instead of loops. Gives significant speed up for larger arrays\n",
        "        (~100x). This was previously the bottleneck since the function is called for every entry in the array.\n",
        "\n",
        "        boxes_list: shape: (N, label, score, weight, model index, x, y)\n",
        "        new_box: shape: (label, score, weight, model index, x, y)\n",
        "    \"\"\"\n",
        "    def bb_iou_array(boxes, new_box):\n",
        "        '''\n",
        "        boxes: shape: (N, x, y)\n",
        "        new_box: shape: (x, y)\n",
        "        '''\n",
        "        # bb interesection over union\n",
        "        x_min = np.minimum(boxes[:, 0], new_box[0])\n",
        "        x_max = np.maximum(boxes[:, 0], new_box[0])\n",
        "        y_min = np.minimum(boxes[:, 1], new_box[1])+1\n",
        "        y_max = np.maximum(boxes[:, 1], new_box[1])+1\n",
        "\n",
        "        iou = np.maximum(0, (y_min-x_max)/(y_max-x_min))\n",
        "\n",
        "        return iou\n",
        "\n",
        "    if boxes_list.shape[0] == 0:\n",
        "        return -1, match_iou\n",
        "\n",
        "    # boxes = np.array(boxes_list)\n",
        "    boxes = boxes_list\n",
        "\n",
        "    ious = bb_iou_array(boxes[:, 4:], new_box[4:])\n",
        "\n",
        "    ious[boxes[:, 0] != new_box[0]] = -1\n",
        "\n",
        "    best_idx = np.argmax(ious)\n",
        "    best_iou = ious[best_idx]\n",
        "\n",
        "    if best_iou <= match_iou:\n",
        "        best_iou = match_iou\n",
        "        best_idx = -1\n",
        "\n",
        "    return best_idx, best_iou\n",
        "\n",
        "\n",
        "def weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=None, iou_thr=0.55, skip_box_thr=0.0, conf_type='avg', allows_overflow=False):\n",
        "    '''\n",
        "    :param boxes_list: list of boxes predictions from each model, each box is 2 numbers.\n",
        "     It has 3 dimensions (models_number, model_preds, 2)\n",
        "     Order of boxes: x, y.\n",
        "    :param scores_list: list of scores for each model\n",
        "    :param labels_list: list of labels for each model\n",
        "    :param weights: list of weights for each model. Default: None, which means weight == 1 for each model\n",
        "    :param iou_thr: IoU value for boxes to be a match\n",
        "    :param skip_box_thr: exclude boxes with score lower than this variable\n",
        "    :param conf_type: how to calculate confidence in weighted boxes. 'avg': average value, 'max': maximum value, 'box_and_model_avg': box and model wise hybrid weighted average, 'absent_model_aware_avg': weighted average that takes into account the absent model.\n",
        "    :param allows_overflow: false if we want confidence score not exceed 1.0\n",
        "\n",
        "    :return: boxes: boxes coordinates (Order of boxes: x, y).\n",
        "    :return: scores: confidence scores\n",
        "    :return: labels: boxes labels\n",
        "    '''\n",
        "\n",
        "    if weights is None:\n",
        "        weights = np.ones(len(boxes_list))\n",
        "    if len(weights) != len(boxes_list):\n",
        "        print('Warning: incorrect number of weights {}. Must be: {}. Set weights equal to 1.'.format(len(weights), len(boxes_list)))\n",
        "        weights = np.ones(len(boxes_list))\n",
        "    weights = np.array(weights)\n",
        "\n",
        "    if conf_type not in ['avg', 'max', 'box_and_model_avg', 'absent_model_aware_avg']:\n",
        "        print('Unknown conf_type: {}. Must be \"avg\", \"max\" or \"box_and_model_avg\", or \"absent_model_aware_avg\"'.format(conf_type))\n",
        "        exit()\n",
        "\n",
        "    filtered_boxes = prefilter_boxes(boxes_list, scores_list, labels_list, weights, skip_box_thr)\n",
        "    if len(filtered_boxes) == 0:\n",
        "        return np.zeros((0, 2)), np.zeros((0,)), np.zeros((0,))\n",
        "\n",
        "    overall_boxes = []\n",
        "    for label in filtered_boxes:\n",
        "        boxes = filtered_boxes[label]\n",
        "        new_boxes = []\n",
        "        weighted_boxes = np.empty((0,6)) ## [label, score, weight, model index, x, y]\n",
        "        # Clusterize boxes\n",
        "        for j in range(0, len(boxes)):\n",
        "            index, best_iou = find_matching_box_quickly(weighted_boxes, boxes[j], iou_thr)\n",
        "\n",
        "            if index != -1:\n",
        "                new_boxes[index].append(boxes[j])\n",
        "                weighted_boxes[index] = get_weighted_box(new_boxes[index], conf_type)\n",
        "            else:\n",
        "                new_boxes.append([boxes[j].copy()])\n",
        "                weighted_boxes = np.vstack((weighted_boxes, boxes[j].copy()))\n",
        "\n",
        "        # Rescale confidence based on number of models and boxes\n",
        "        for i in range(len(new_boxes)):\n",
        "            clustered_boxes = np.array(new_boxes[i])\n",
        "            if conf_type == 'box_and_model_avg':\n",
        "                # weighted average for boxes\n",
        "                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weighted_boxes[i, 2]\n",
        "                # identify unique model index by model index column\n",
        "                _, idx = np.unique(clustered_boxes[:, 3], return_index=True)\n",
        "                # rescale by unique model weights\n",
        "                weighted_boxes[i, 1] = weighted_boxes[i, 1] *  clustered_boxes[idx, 2].sum() / weights.sum()\n",
        "            elif conf_type == 'absent_model_aware_avg':\n",
        "                # get unique model index in the cluster\n",
        "                models = np.unique(clustered_boxes[:, 3]).astype(int)\n",
        "                # create a mask to get unused model weights\n",
        "                mask = np.ones(len(weights), dtype=bool)\n",
        "                mask[models] = False\n",
        "                # absent model aware weighted average\n",
        "                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / (weighted_boxes[i, 2] + weights[mask].sum())\n",
        "            elif conf_type == 'max':\n",
        "                weighted_boxes[i, 1] = weighted_boxes[i, 1] / weights.max()\n",
        "            elif not allows_overflow:\n",
        "                weighted_boxes[i, 1] = weighted_boxes[i, 1] * min(len(weights), len(clustered_boxes)) / weights.sum()\n",
        "            else:\n",
        "                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weights.sum()\n",
        "\n",
        "        # REQUIRE BBOX TO BE PREDICTED BY AT LEAST 2 MODELS\n",
        "        #for i in range(len(new_boxes)):\n",
        "        #    clustered_boxes = np.array(new_boxes[i])\n",
        "        #    if len(np.unique(clustered_boxes[:, 3])) > 1:\n",
        "        #        overall_boxes.append(weighted_boxes[i])\n",
        "\n",
        "        overall_boxes.append(weighted_boxes) # NOT NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n",
        "    overall_boxes = np.concatenate(overall_boxes, axis=0) # NOT NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n",
        "    #overall_boxes = np.array(overall_boxes) # NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n",
        "    overall_boxes = overall_boxes[overall_boxes[:, 1].argsort()[::-1]]\n",
        "    boxes = overall_boxes[:, 4:]\n",
        "    scores = overall_boxes[:, 1]\n",
        "    labels = overall_boxes[:, 0]\n",
        "    return boxes, scores, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2023-01-25T19:49:05.924318Z",
          "iopub.status.busy": "2023-01-25T19:49:05.923872Z",
          "iopub.status.idle": "2023-01-25T20:00:59.775651Z",
          "shell.execute_reply": "2023-01-25T20:00:59.774749Z",
          "shell.execute_reply.started": "2023-01-25T19:49:05.924282Z"
        },
        "id": "Ff-5-kECVg0R",
        "outputId": "3204344d-0e4f-40cd-8be5-7a4dc9eacdcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: ./input/longformerwithbilstmhead/model.py: No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/Volumes/GoogleDrive/My Drive/%% RESEARCH PROJECT %%/ALTUS GRANT WORK/generate_preds.py\", line 52, in <module>\n",
            "    from model import LongformerForTokenClassificationwithbiLSTM\n",
            "  File \"/Volumes/GoogleDrive/My Drive/%% RESEARCH PROJECT %%/ALTUS GRANT WORK/model.py\", line 1, in <module>\n",
            "    from transformers.models.deberta.modeling_deberta import DebertaPreTrainedModel, DebertaModel\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/__init__.py\", line 19, in <module>\n",
            "    from . import (\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/mt5/__init__.py\", line 40, in <module>\n",
            "    from ..t5.tokenization_t5_fast import T5TokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py\", line 23, in <module>\n",
            "    from ...tokenization_utils_fast import PreTrainedTokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\", line 31, in <module>\n",
            "    from .convert_slow_tokenizer import convert_slow_tokenizer\n",
            "ModuleNotFoundError: No module named 'transformers.convert_slow_tokenizer'\n",
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1063, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 972, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/__init__.py\", line 19, in <module>\n",
            "    from . import (\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/mt5/__init__.py\", line 40, in <module>\n",
            "    from ..t5.tokenization_t5_fast import T5TokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py\", line 23, in <module>\n",
            "    from ...tokenization_utils_fast import PreTrainedTokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\", line 31, in <module>\n",
            "    from .convert_slow_tokenizer import convert_slow_tokenizer\n",
            "ModuleNotFoundError: No module named 'transformers.convert_slow_tokenizer'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Volumes/GoogleDrive/My Drive/%% RESEARCH PROJECT %%/ALTUS GRANT WORK/generate_preds.py\", line 66, in <module>\n",
            "    from transformers import (AutoModelForTokenClassification, \n",
            "  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1053, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1065, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.models.auto because of the following error (look up to see its traceback):\n",
            "No module named 'transformers.convert_slow_tokenizer'\n",
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1063, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 972, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/__init__.py\", line 19, in <module>\n",
            "    from . import (\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/mt5/__init__.py\", line 40, in <module>\n",
            "    from ..t5.tokenization_t5_fast import T5TokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py\", line 23, in <module>\n",
            "    from ...tokenization_utils_fast import PreTrainedTokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\", line 31, in <module>\n",
            "    from .convert_slow_tokenizer import convert_slow_tokenizer\n",
            "ModuleNotFoundError: No module named 'transformers.convert_slow_tokenizer'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Volumes/GoogleDrive/My Drive/%% RESEARCH PROJECT %%/ALTUS GRANT WORK/generate_preds.py\", line 66, in <module>\n",
            "    from transformers import (AutoModelForTokenClassification, \n",
            "  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1053, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1065, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.models.auto because of the following error (look up to see its traceback):\n",
            "No module named 'transformers.convert_slow_tokenizer'\n",
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1063, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 972, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/__init__.py\", line 19, in <module>\n",
            "    from . import (\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/mt5/__init__.py\", line 40, in <module>\n",
            "    from ..t5.tokenization_t5_fast import T5TokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py\", line 23, in <module>\n",
            "    from ...tokenization_utils_fast import PreTrainedTokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\", line 31, in <module>\n",
            "    from .convert_slow_tokenizer import convert_slow_tokenizer\n",
            "ModuleNotFoundError: No module named 'transformers.convert_slow_tokenizer'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Volumes/GoogleDrive/My Drive/%% RESEARCH PROJECT %%/ALTUS GRANT WORK/generate_preds.py\", line 66, in <module>\n",
            "    from transformers import (AutoModelForTokenClassification, \n",
            "  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1053, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1065, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.models.auto because of the following error (look up to see its traceback):\n",
            "No module named 'transformers.convert_slow_tokenizer'\n",
            "Traceback (most recent call last):\n",
            "  File \"/Volumes/GoogleDrive/My Drive/%% RESEARCH PROJECT %%/ALTUS GRANT WORK/generate_preds.py\", line 40, in <module>\n",
            "    shutil.copy(convert_file, transformers_path)\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/shutil.py\", line 427, in copy\n",
            "    copyfile(src, dst, follow_symlinks=follow_symlinks)\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/shutil.py\", line 264, in copyfile\n",
            "    with open(src, 'rb') as fsrc:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'input/deberta-v2-3-fast-tokenizer/convert_slow_tokenizer.py'\n",
            "cp: ./input/deberta-lstm/model.py: No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/Volumes/GoogleDrive/My Drive/%% RESEARCH PROJECT %%/ALTUS GRANT WORK/generate_preds.py\", line 56, in <module>\n",
            "    from model import DebertaForTokenClassificationwithbiLSTM\n",
            "  File \"/Volumes/GoogleDrive/My Drive/%% RESEARCH PROJECT %%/ALTUS GRANT WORK/model.py\", line 1, in <module>\n",
            "    from transformers.models.deberta.modeling_deberta import DebertaPreTrainedModel, DebertaModel\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/__init__.py\", line 19, in <module>\n",
            "    from . import (\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/mt5/__init__.py\", line 40, in <module>\n",
            "    from ..t5.tokenization_t5_fast import T5TokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py\", line 23, in <module>\n",
            "    from ...tokenization_utils_fast import PreTrainedTokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\", line 31, in <module>\n",
            "    from .convert_slow_tokenizer import convert_slow_tokenizer\n",
            "ModuleNotFoundError: No module named 'transformers.convert_slow_tokenizer'\n",
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1063, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 972, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/__init__.py\", line 19, in <module>\n",
            "    from . import (\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/mt5/__init__.py\", line 40, in <module>\n",
            "    from ..t5.tokenization_t5_fast import T5TokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py\", line 23, in <module>\n",
            "    from ...tokenization_utils_fast import PreTrainedTokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\", line 31, in <module>\n",
            "    from .convert_slow_tokenizer import convert_slow_tokenizer\n",
            "ModuleNotFoundError: No module named 'transformers.convert_slow_tokenizer'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Volumes/GoogleDrive/My Drive/%% RESEARCH PROJECT %%/ALTUS GRANT WORK/generate_preds.py\", line 66, in <module>\n",
            "    from transformers import (AutoModelForTokenClassification, \n",
            "  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1053, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1065, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.models.auto because of the following error (look up to see its traceback):\n",
            "No module named 'transformers.convert_slow_tokenizer'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1063, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 972, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/__init__.py\", line 19, in <module>\n",
            "    from . import (\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/mt5/__init__.py\", line 40, in <module>\n",
            "    from ..t5.tokenization_t5_fast import T5TokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py\", line 23, in <module>\n",
            "    from ...tokenization_utils_fast import PreTrainedTokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\", line 31, in <module>\n",
            "    from .convert_slow_tokenizer import convert_slow_tokenizer\n",
            "ModuleNotFoundError: No module named 'transformers.convert_slow_tokenizer'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Volumes/GoogleDrive/My Drive/%% RESEARCH PROJECT %%/ALTUS GRANT WORK/generate_preds.py\", line 66, in <module>\n",
            "    from transformers import (AutoModelForTokenClassification, \n",
            "  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1053, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1065, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.models.auto because of the following error (look up to see its traceback):\n",
            "No module named 'transformers.convert_slow_tokenizer'\n",
            "cp: ./input/hf-transformers/transformers-4.16.0: No such file or directory\n",
            "\u001b[31mERROR: Invalid requirement: '/kaggle/working/transformers-4.16.0'\n",
            "Hint: It looks like a path. File '/kaggle/working/transformers-4.16.0' does not exist.\u001b[0m\n",
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1063, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 972, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/__init__.py\", line 19, in <module>\n",
            "    from . import (\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/mt5/__init__.py\", line 40, in <module>\n",
            "    from ..t5.tokenization_t5_fast import T5TokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py\", line 23, in <module>\n",
            "    from ...tokenization_utils_fast import PreTrainedTokenizerFast\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\", line 31, in <module>\n",
            "    from .convert_slow_tokenizer import convert_slow_tokenizer\n",
            "ModuleNotFoundError: No module named 'transformers.convert_slow_tokenizer'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Volumes/GoogleDrive/My Drive/%% RESEARCH PROJECT %%/ALTUS GRANT WORK/generate_preds.py\", line 66, in <module>\n",
            "    from transformers import (AutoModelForTokenClassification, \n",
            "  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1053, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1065, in _get_module\n",
            "    raise RuntimeError(\n",
            "RuntimeError: Failed to import transformers.models.auto because of the following error (look up to see its traceback):\n",
            "No module named 'transformers.convert_slow_tokenizer'\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'longformerwithlstm.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/nw/fw9qknzj1qn1t99hfbbv690mqjvj6y/T/ipykernel_15284/1222896111.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mlongformer_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"longformerwithlstm.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mdeberta_v3_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"debertawithlstm.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdeberta_v2_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"deberta_v2.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'longformerwithlstm.csv'"
          ]
        }
      ],
      "source": [
        "!python generate_preds.py --model_paths ./input/longformerwithbilstmhead/aug-longformer-large-4096-f0/checkpoint-5500 \\\n",
        "                                        ./input/longformerwithbilstmhead/aug-longformer-large-4096-f2/checkpoint-7500 \\\n",
        "                                        ./input/longformerwithbilstmhead/aug-longformer-large-4096-f5/checkpoint-6000 \\\n",
        "                            --save_name longformerwithlstm --max_len 1536\n",
        "\n",
        "!python generate_preds.py --model_paths ./input/deberta-large-100/fold0 \\\n",
        "                                        ./input/deberta-large-100/fold1 \\\n",
        "                                        ./input/deberta-large-100/fold2 \\\n",
        "                            --save_name debertal_chris --max_len 1536\n",
        "\n",
        "!python generate_preds.py --model_paths ./input/deberta-large-v2/deberta-large-v2100-f0/checkpoint-10500 \\\n",
        "                                        ./input/deberta-large-v2/deberta-large-v2101-f1/checkpoint-11500 \\\n",
        "                                    ./input/deberta-large-v2/deberta-large-v2102-f2/checkpoint-8500 \\\n",
        "                            --save_name debertal --max_len 1536\n",
        "\n",
        "!python generate_preds.py --model_paths ./input/deberta-xlarge-1536/deberta-xlarge-v8004-f4/checkpoint-14000 \\\n",
        "                                        ./input/deberta-xlarge-1536/deberta-xlarge-v4005-f5/checkpoint-13000 \\\n",
        "                            --save_name debertaxl --max_len 1536\n",
        "\n",
        "!python generate_preds.py --model_paths ./input/deberta-v2-xlarge/deberta-v2-xlarge-v6000-f0/checkpoint-7500 \\\n",
        "                                        ./input/deberta-v2-xlarge/deberta-v2-xlarge-v6003-f3/checkpoint-9000 \\\n",
        "                            --save_name deberta_v2 --max_len 1536\n",
        "\n",
        "!python generate_preds.py --model_paths ./input/deberta-lstm-jaccard/jcl-deberta-large-f1/checkpoint-4500 \\\n",
        "                                        ./input/deberta-lstm-jaccard/jcl-deberta-large-f2/checkpoint-5000 \\\n",
        "                                        ./input/deberta-lstm-jaccard/jcl-deberta-large-f3/checkpoint-3500 \\\n",
        "                            --save_name debertawithlstm --max_len 1536\n",
        "\n",
        "!python generate_preds.py --model_paths  ./input/funnel-large-6folds/large-v628-f1/checkpoint-11500 \\\n",
        "                                         ./input/funnel-large-6folds/large-v627-f3/checkpoint-11000 \\\n",
        "                                         ./input/funnel-large-6folds/large-v623-f4/checkpoint-10500 \\\n",
        "                            --save_name funnel --max_len 1536\n",
        "\n",
        "#!python generate_preds.py --model_paths  ./input/auglsgrobertalarge/lsg-roberta-large-0/checkpoint-6750 \\\n",
        "#                                         ./input/auglsgrobertalarge/lsg-roberta-large-2/checkpoint-7000 \\\n",
        "#                                         ./input/auglsgrobertalarge/lsg-roberta-large-5/checkpoint-6500 \\\n",
        "#                             --save_name lsg --max_len 1536\n",
        "\n",
        "!python generate_preds.py --model_paths  ./input/bird-base/fold1 \\\n",
        "                                         ./input/bird-base/fold3 \\\n",
        "                                         ./input/bird-base/fold5 \\\n",
        "                            --save_name bigbird_base_chris --max_len 1024\n",
        "\n",
        "!python generate_preds.py --model_paths  ./input/feedbackyoso/yoso-4096-0/checkpoint-12500 \\\n",
        "                                         ./input/feedbackyoso/yoso-4096-2/checkpoint-11000 \\\n",
        "                                         ./input/feedbackyoso/yoso-4096-4/checkpoint-12500 \\\n",
        "                            --save_name yoso --max_len 1536\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np, os\n",
        "\n",
        "longformer_csv = pd.read_csv(\"longformerwithlstm.csv\").dropna()\n",
        "deberta_v3_csv = pd.read_csv(\"debertawithlstm.csv\").dropna()\n",
        "deberta_v2_csv = pd.read_csv(\"deberta_v2.csv\").dropna()\n",
        "debertaxl_csv = pd.read_csv(\"debertaxl.csv\").dropna()\n",
        "debertal_chris_csv = pd.read_csv(\"debertal_chris.csv\").dropna()\n",
        "debertal_csv = pd.read_csv(\"debertal.csv\").dropna()\n",
        "yoso_csv = pd.read_csv(\"yoso.csv\").dropna()\n",
        "funnel_csv = pd.read_csv(\"funnel.csv\").dropna()\n",
        "bird_base_chris_csv = pd.read_csv(\"bigbird_base_chris.csv\").dropna()\n",
        "#lsg_csv = pd.read_csv(\"lsg.csv\").dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZKKS_z9Vg0S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy as np, os\n",
        "path = './Interim results/'\n",
        "longformer_csv = pd.read_csv(path+\"longformerwithlstm.csv\").dropna()\n",
        "deberta_v3_csv = pd.read_csv(path+\"debertawithlstm.csv\").dropna()\n",
        "deberta_v2_csv = pd.read_csv(path+\"deberta_v2.csv\").dropna()\n",
        "debertaxl_csv = pd.read_csv(path+\"debertaxl.csv\").dropna()\n",
        "debertal_chris_csv = pd.read_csv(path+\"debertal_chris.csv\").dropna()\n",
        "debertal_csv = pd.read_csv(path+\"debertal.csv\").dropna()\n",
        "yoso_csv = pd.read_csv(path+\"yoso.csv\").dropna()\n",
        "#funnel_csv = pd.read_csv(\"funnel.csv\").dropna()\n",
        "bird_base_chris_csv = pd.read_csv(path+\"bigbird_base_chris.csv\").dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDagiJ_EVg0S"
      },
      "source": [
        "\n",
        "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Mar-2022/conf_scores.png)\n",
        "\n",
        "[1]: https://www.kaggle.com/c/tensorflow-great-barrier-reef/discussion/307609"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svEolJeRVg0S",
        "outputId": "5aef2b61-49af-4d4b-aa8c-d6270d67680f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i1_23ctkCD6pbpEvNaLb', 'i1_23ctkCD6pbpEvNaLb', 'i1_23ctkCD6pbpEvNaLb', 'i1_25XSiAHGKcjwL7h6A', 'i1_25XSiAHGKcjwL7h6A', 'i1_25xc5mgS4Ldkuj8a7', 'i1_25xc5mgS4Ldkuj8a7', 'i1_25xc5mgS4Ldkuj8a7', 'i1_27LpDGxTTjEmhC7MT', 'i1_29FNyqKMGGsnuzuSX']\n"
          ]
        }
      ],
      "source": [
        "#TEST_DIR = './test_input/'\n",
        "#test_files = os.listdir(TEST_DIR)\n",
        "\n",
        "v_ids = pd.read_csv('submission.csv')\n",
        "v_ids = v_ids['id'].tolist()\n",
        "#v_ids = [f.replace('.txt','') for f in test_files]\n",
        "print(v_ids[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jctxWfJYVg0T"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class_to_label = {\n",
        "    'Claim': 0,\n",
        "    'Evidence': 1,\n",
        "    'Lead':2,\n",
        "    'Position':3,\n",
        "    'Concluding Statement':4,\n",
        "    'Counterclaim':5,\n",
        "    'Rebuttal':6\n",
        "}\n",
        "\n",
        "# Threshold found from CV\n",
        "label_to_threshold = {\n",
        "    0 : 0.275, #Claim\n",
        "    1 : 0.375, #Evidence\n",
        "    2 : 0.325, #Lead\n",
        "    3 : 0.325, #Position\n",
        "    4 : 0.4, #Concluding Statement\n",
        "    5 : 0.275, #Counterclaim\n",
        "    6 : 0.275 #Rebuttal\n",
        "}\n",
        "\n",
        "label_to_class = {v:k for k, v in class_to_label.items()}\n",
        "\n",
        "def preprocess_for_wbf(df_list):\n",
        "    boxes_list=[]\n",
        "    scores_list=[]\n",
        "    labels_list=[]\n",
        "\n",
        "    for df in df_list:\n",
        "        scores_list.append(df['scores'].values.tolist())\n",
        "        labels_list.append(df['class'].map(class_to_label).values.tolist())\n",
        "        predictionstring = df.predictionstring.str.split().values\n",
        "        df_box_list = []\n",
        "        for bb in predictionstring:\n",
        "            df_box_list.append([int(bb[0]), int(bb[-1])])\n",
        "        boxes_list.append(df_box_list)\n",
        "    return boxes_list, scores_list, labels_list\n",
        "\n",
        "def postprocess_for_wbf(idx, boxes_list, scores_list, labels_list):\n",
        "    preds = []\n",
        "    for box, score, label in zip(boxes_list, scores_list, labels_list):\n",
        "        if score > label_to_threshold[label]:\n",
        "            start = math.ceil(box[0])\n",
        "            end = int(box[1])\n",
        "            preds.append((idx, label_to_class[label], ' '.join([str(x) for x in range(start, end+1)]), score))\n",
        "    return preds\n",
        "\n",
        "def generate_wbf_for_id(i):\n",
        "    df1 = debertal_csv[debertal_csv['id']==i]\n",
        "    df2 = debertal_chris_csv[debertal_chris_csv['id']==i]\n",
        "    #df3 = funnel_csv[funnel_csv['id']==i]\n",
        "    df3 = debertaxl_csv[debertaxl_csv['id']==i]\n",
        "    df4 = longformer_csv[longformer_csv['id']==i]\n",
        "    df5 = deberta_v3_csv[deberta_v3_csv['id']==i]\n",
        "    df6 = yoso_csv[yoso_csv['id']==i]\n",
        "    df7 = bird_base_chris_csv[bird_base_chris_csv['id']==i]\n",
        "    #df8 = lsg_csv[lsg_csv['id']==i]\n",
        "    df9 = deberta_v2_csv[deberta_v2_csv['id']==i]\n",
        "\n",
        "    boxes_list, scores_list, labels_list = preprocess_for_wbf([df1, df2, df3, df4, df5, df6, df7, df9])\n",
        "    nboxes_list, nscores_list, nlabels_list = weighted_boxes_fusion(boxes_list, scores_list, labels_list, iou_thr=0.33, conf_type='avg')\n",
        "\n",
        "    return postprocess_for_wbf(i, nboxes_list, nscores_list, nlabels_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ewf6WevVg0T"
      },
      "outputs": [],
      "source": [
        "import multiprocess as mp\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with mp.Pool(2) as p:\n",
        "        list_of_list = p.map(generate_wbf_for_id, v_ids)\n",
        "\n",
        "preds = [x for sub_list in list_of_list for x in sub_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0Dw4jxRVg0U"
      },
      "outputs": [],
      "source": [
        "sub = pd.DataFrame(preds)\n",
        "sub.columns = [\"id\", \"class\", \"predictionstring\", \"scores\"]\n",
        "sub.to_csv('submission2.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Am_DXpRVg0U",
        "outputId": "ea0ca4e6-8087-425f-8513-2aac4671b55e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6375329068644192"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sub[sub['class']=='Position']['scores'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe3RllIeVg0U"
      },
      "outputs": [],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTP6CS4mVg0V"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}